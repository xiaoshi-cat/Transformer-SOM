import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import accuracy_score
from minisom import MiniSom
import os
import shutil
import warnings

# ==========================================
# 0. å…¨å±€é…ç½®
# ==========================================
# å¿½ç•¥ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore")
# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ (å¦‚æœç¯å¢ƒæ”¯æŒ)
plt.rcParams['axes.unicode_minus'] = False

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "Experiment_Results_Final"
if os.path.exists(OUTPUT_DIR):
    shutil.rmtree(OUTPUT_DIR)
os.makedirs(OUTPUT_DIR)
print(f"æ–‡ä»¶å¤¹ '{OUTPUT_DIR}' å·²åˆ›å»ºï¼Œæ‰€æœ‰ç»“æœå°†ä¿å­˜åœ¨æ­¤ã€‚")


# ==========================================
# 1. æ¨¡å‹å®šä¹‰: å¾®å‹ Supervised Transformer
# ==========================================
class InterpretableEncoderLayer(nn.Module):
    """
    å¯è§£é‡Šçš„ç¼–ç å±‚ï¼šåœ¨ Forward è¿‡ç¨‹ä¸­ä¿å­˜ Attention æƒé‡
    """

    def __init__(self, d_model, nhead, dim_feedforward, dropout):
        super().__init__()
        # average_attn_weights=True: è¿”å› [Batch, Seq, Seq]
        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)

        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.last_attn_weights = None  # ç”¨äºå­˜å‚¨æ³¨æ„åŠ›æƒé‡

    def forward(self, src):
        # src: [Batch, Seq, Feature_Dim]
        src2, weights = self.self_attn(src, src, src, need_weights=True, average_attn_weights=True)
        self.last_attn_weights = weights

        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src


class SupervisedTransformerAE(nn.Module):
    """
    é’ˆå¯¹å°æ ·æœ¬ä¼˜åŒ–çš„ Supervised Autoencoder
    """

    def __init__(self, num_features, num_classes, d_model=16, nhead=2, num_layers=1):
        super(SupervisedTransformerAE, self).__init__()
        # ç‰¹å¾åµŒå…¥å±‚
        self.feature_embedding = nn.Linear(1, d_model)
        self.column_embedding = nn.Parameter(torch.randn(1, num_features, d_model))

        # ç¼–ç å™¨ (å±‚æ•°å‡å°‘åˆ°1ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ)
        self.layers = nn.ModuleList([
            InterpretableEncoderLayer(d_model, nhead, dim_feedforward=32, dropout=0.2)
            for _ in range(num_layers)
        ])

        # è§£ç å™¨ (é‡å»ºä»»åŠ¡)
        self.decoder = nn.Sequential(
            nn.Linear(d_model, 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )

        # åˆ†ç±»å¤´ (ç›‘ç£ä»»åŠ¡ï¼Œæ¶ˆèå®éªŒ Baseline)
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(num_features * d_model, 32),
            nn.ReLU(),
            nn.Dropout(0.3),  # é«˜ Dropout å¢åŠ é²æ£’æ€§
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        # [Batch, Features] -> [Batch, Features, 1]
        x_emb = self.feature_embedding(x.unsqueeze(-1)) + self.column_embedding

        latent = x_emb
        for layer in self.layers:
            latent = layer(latent)

        # 1. é‡å»º
        reconstruction = self.decoder(latent).squeeze(-1)
        # 2. åˆ†ç±» (Logits)
        class_logits = self.classifier(latent)

        return reconstruction, latent, class_logits


# ==========================================
# 2. æ ¸å¿ƒç»˜å›¾å‡½æ•°
# ==========================================

def plot_attention_heatmap(model, data_tensor, feature_names):
    """ç»˜åˆ¶è‡ªæ³¨æ„åŠ›çƒ­åŠ›å›¾ (çº¢è‰²ç³»)"""
    print(">>> æ­£åœ¨ç»˜åˆ¶ [è‡ªæ³¨æ„åŠ›çƒ­åŠ›å›¾]...")
    model.eval()
    with torch.no_grad():
        model(data_tensor)

    # è·å–ç¬¬ä¸€å±‚çš„å¹³å‡æ³¨æ„åŠ›æƒé‡
    attn_weights = model.layers[0].last_attn_weights.mean(dim=0).cpu().numpy()

    plt.figure(figsize=(10, 9))
    sns.heatmap(attn_weights, xticklabels=feature_names, yticklabels=feature_names,
                cmap='Reds', annot=False, square=True,
                cbar_kws={'label': 'Attention Weight (Importance)', 'shrink': 0.8})

    plt.title('Self-Attention Heatmap (Global Interpretability)', fontsize=14, fontweight='bold')
    plt.xlabel('Source Feature (Key)', fontsize=12)
    plt.ylabel('Target Feature (Query)', fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'Advanced_1_Attention_Heatmap.png'), dpi=300)


def plot_som_component_planes(som, latent_features, raw_features_data, feature_names):
    """
    ç»˜åˆ¶ SOM ç»„ä»¶å¹³é¢å›¾
    å…³é”®ç‚¹ï¼šä¼ å…¥çš„æ˜¯ Z-score æ ‡å‡†åŒ–åçš„æ•°æ® (raw_features_data)
    """
    print(">>> æ­£åœ¨ç»˜åˆ¶ [SOM ç»„ä»¶å¹³é¢å›¾ (Z-score)]...")
    weights_shape = som.get_weights().shape[:2]  # (6, 6)

    # å‡†å¤‡å®¹å™¨
    component_planes = np.zeros((weights_shape[0], weights_shape[1], len(feature_names)))
    counts = np.zeros(weights_shape)

    # å…¼å®¹å¤„ç†: ç¡®ä¿æ˜¯ numpy array
    if isinstance(raw_features_data, pd.DataFrame):
        raw_values = raw_features_data.values
    else:
        raw_values = raw_features_data

        # ç´¯åŠ æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„æ ·æœ¬ç‰¹å¾å€¼
    for i, x in enumerate(latent_features):
        w = som.winner(x)
        component_planes[w] += raw_values[i]
        counts[w] += 1

    # æ±‚å¹³å‡
    global_means = np.mean(raw_values, axis=0)
    for r in range(weights_shape[0]):
        for c in range(weights_shape[1]):
            if counts[r, c] > 0:
                component_planes[r, c] /= counts[r, c]
            else:
                component_planes[r, c] = global_means  # ç©ºèŠ‚ç‚¹å¡«å‡å€¼

    # ç»˜å›¾ 3x3
    fig, axes = plt.subplots(3, 3, figsize=(15, 14))
    axes = axes.flatten()

    for i, name in enumerate(feature_names):
        if i >= len(axes): break
        # ä½¿ç”¨ coolwarm, é¢œè‰²æ¡æ ‡ç­¾è®¾ä¸º Z-score
        sns.heatmap(component_planes[:, :, i], ax=axes[i], cmap='coolwarm',
                    annot=False, cbar=True, square=True,
                    cbar_kws={'label': 'Z-score', 'shrink': 0.8})
        axes[i].set_title(f'{name} Distribution', fontsize=12, fontweight='bold')
        axes[i].axis('off')

    plt.suptitle('SOM Component Planes (Z-score Standardized)', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(OUTPUT_DIR, 'Advanced_2_SOM_Components_Zscore.png'), dpi=300)


def plot_latent_space_comparison(X_raw, X_latent, y, class_names):
    """ç»˜åˆ¶éšç©ºé—´å¯¹æ¯”å›¾ (Raw PCA vs Latent PCA vs Latent t-SNE)"""
    print(">>> æ­£åœ¨ç»˜åˆ¶ [éšç©ºé—´åˆ†å¸ƒå¯¹æ¯”å›¾]...")

    # 1. Raw Data PCA
    pca = PCA(n_components=2)
    X_raw_pca = pca.fit_transform(X_raw)

    # 2. Latent Data PCA
    X_latent_pca = pca.fit_transform(X_latent)

    # 3. Latent Data t-SNE (é€‚é…å°æ ·æœ¬çš„ perplexity)
    tsne = TSNE(n_components=2, perplexity=min(10, len(X_raw) - 1), random_state=42)
    X_latent_tsne = tsne.fit_transform(X_latent)

    fig, axes = plt.subplots(1, 3, figsize=(24, 7))
    colors = sns.color_palette("husl", len(class_names))

    plot_data = [
        (X_raw_pca, 'Raw Data (PCA)'),
        (X_latent_pca, 'Latent Space (PCA)'),
        (X_latent_tsne, 'Latent Space (t-SNE)')
    ]

    for ax_idx, (data, title) in enumerate(plot_data):
        ax = axes[ax_idx]
        for i, name in enumerate(class_names):
            mask = (y == i)
            ax.scatter(data[mask, 0], data[mask, 1], label=name,
                       color=colors[i], s=60, alpha=0.8, edgecolors='white')
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend(loc='best')
        ax.grid(True, linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'Advanced_3_Latent_Space_Comparison.png'), dpi=300)


def plot_u_matrix(som, data, y, class_names):
    """ç»˜åˆ¶ U-Matrix (å›¾ä¾‹åœ¨åº•éƒ¨)"""
    print(">>> æ­£åœ¨ç»˜åˆ¶ [U-Matrix èšç±»å›¾]...")
    plt.figure(figsize=(10, 10))

    u_matrix = som.distance_map()

    # èƒŒæ™¯è·ç¦»çƒ­åŠ›å›¾
    sns.heatmap(u_matrix, cmap='coolwarm', annot=False,
                cbar_kws={'label': 'Euclidean Distance (Blue=Center, Red=Boundary)', 'shrink': 0.8},
                square=True)

    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*']
    colors = sns.color_palette("husl", len(class_names))

    w_x, w_y = [], []
    for x in data:
        w = som.winner(x)
        w_x.append(w[0]);
        w_y.append(w[1])
    w_x = np.array(w_x);
    w_y = np.array(w_y)

    # ç»˜åˆ¶æ•£ç‚¹ (å¸¦æŠ–åŠ¨é˜²æ­¢é‡å )
    for i, name in enumerate(class_names):
        idx = np.where(y == i)[0]
        jitter_x = np.random.rand(len(idx)) * 0.6 - 0.3
        jitter_y = np.random.rand(len(idx)) * 0.6 - 0.3

        plt.scatter(w_y[idx] + 0.5 + jitter_y, w_x[idx] + 0.5 + jitter_x,
                    label=name, s=60, color=colors[i], marker=markers[i % len(markers)],
                    edgecolors='white', linewidth=1.0, alpha=0.9)

    plt.title('U-Matrix with Sample Distribution', fontsize=15, fontweight='bold', pad=20)
    # å›¾ä¾‹æ”¾åº•éƒ¨
    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.08),
               fancybox=True, shadow=True, ncol=4, borderaxespad=0.)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'Advanced_4_U_Matrix.png'), dpi=300)


def plot_ablation_study(trans_acc, som_acc):
    """ç»˜åˆ¶æ¶ˆèå®éªŒå¯¹æ¯”å›¾"""
    print(">>> æ­£åœ¨ç»˜åˆ¶ [æ¶ˆèå®éªŒå¯¹æ¯”å›¾]...")
    plt.figure(figsize=(7, 6))
    methods = ['Transformer Head\n(Baseline)', 'Transformer + SOM\n(Ours)']
    accs = [trans_acc, som_acc]
    colors = ['gray', '#e74c3c']  # ç°è‰²å¯¹æ¯”çº¢è‰²

    bars = plt.bar(methods, accs, color=colors, width=0.6, alpha=0.9)

    plt.ylabel('Accuracy', fontsize=12)
    plt.title('Ablation Study: Classifier vs. SOM', fontsize=14, fontweight='bold')
    plt.ylim(0, 1.15)
    plt.grid(axis='y', linestyle='--', alpha=0.5)

    # æ ‡æ•°å€¼
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2., height + 0.02,
                 f'{height:.1%}', ha='center', va='bottom', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'Advanced_5_Ablation_Study.png'), dpi=300)


# ==========================================
# 3. ä¸»ç¨‹åºæµç¨‹
# ==========================================
def main():
    print(">>> [1/7] è¯»å–æ•°æ®...")
    if not os.path.exists('new_train_data.csv'):
        print("âŒ é”™è¯¯: å½“å‰ç›®å½•ä¸‹æœªæ‰¾åˆ° 'new_train_data.csv'")
        return

    train_df = pd.read_csv('new_train_data.csv')
    test_df = pd.read_csv('new_test_data.csv')  # å¦‚æœæœ‰çš„è¯

    feature_cols = ['Ca', 'Mg', 'Na', 'HCO3', 'Cl', 'SO4', 'TH', 'TA', 'PH']
    target_col = 'Label'

    # 1. æ ‡ç­¾ç¼–ç 
    full_labels = pd.concat([train_df[target_col], test_df[target_col]], axis=0)
    le = LabelEncoder()
    le.fit(full_labels)
    class_names = le.classes_

    y_train = le.transform(train_df[target_col])

    # 2. ç‰¹å¾æ ‡å‡†åŒ– (Z-score)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(train_df[feature_cols].values)

    print(f"    - è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)} (Small Data Mode)")
    print(f"    - ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"    - ç±»åˆ«æ•°: {len(class_names)}")

    # å‡†å¤‡ Tensor
    train_tensor = torch.FloatTensor(X_train)
    train_labels = torch.LongTensor(y_train)

    print(">>> [2/7] è®­ç»ƒ Supervised Transformer (Microç‰ˆ)...")
    # åˆå§‹åŒ–æ¨¡å‹: 16ç»´, 1å±‚, 2å¤´
    model = SupervisedTransformerAE(len(feature_cols), len(class_names),
                                    d_model=16, nhead=2, num_layers=1)

    # ä¼˜åŒ–å™¨: åŠ å…¥ Weight Decay é˜²æ­¢è¿‡æ‹Ÿåˆ
    optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-3)

    # æŸå¤±å‡½æ•°: é‡å»º + åˆ†ç±»
    criterion_recon = nn.MSELoss()
    criterion_class = nn.CrossEntropyLoss()

    epochs = 150
    for ep in range(epochs):
        model.train()
        optimizer.zero_grad()

        recon, _, logits = model(train_tensor)

        loss_r = criterion_recon(recon, train_tensor)
        loss_c = criterion_class(logits, train_labels)

        # è”åˆæŸå¤±: åŠ å¤§åˆ†ç±»æƒé‡ (0.8) å¼ºè¿«åˆ†ç¦»
        total_loss = loss_r + 0.8 * loss_c

        total_loss.backward()
        optimizer.step()

        if (ep + 1) % 50 == 0:
            print(f"    Epoch {ep + 1}/{epochs} | Loss: {total_loss.item():.4f}")

    print(">>> [3/7] æå– Latent Features...")
    model.eval()
    with torch.no_grad():
        _, tr_lat, tr_logits = model(train_tensor)
        feat_train = tr_lat.reshape(len(X_train), -1).numpy()

        # --- æ¶ˆèå®éªŒæ•°æ®å‡†å¤‡: è®¡ç®— Transformer è‡ªå¸¦åˆ†ç±»å¤´çš„å‡†ç¡®ç‡ ---
        trans_preds = torch.argmax(tr_logits, dim=1).numpy()
        trans_acc = accuracy_score(y_train, trans_preds)

    print(">>> [4/7] è®­ç»ƒ SOM (æ‹“æ‰‘èšç±»)...")
    som = MiniSom(6, 6, feat_train.shape[1], sigma=1.0, learning_rate=0.5, random_seed=42)
    som.train_random(feat_train, 5000)

    # --- æ¶ˆèå®éªŒæ•°æ®å‡†å¤‡: è®¡ç®— SOM çš„å‡†ç¡®ç‡ ---
    # å»ºç«‹æ˜ å°„: èŠ‚ç‚¹ -> ç±»åˆ«æ ‡ç­¾ (å¤šæ•°æŠ•ç¥¨)
    node_map = {}
    for i, x in enumerate(feat_train):
        w = som.winner(x)
        if w not in node_map: node_map[w] = []
        node_map[w].append(y_train[i])
    for w in node_map: node_map[w] = max(set(node_map[w]), key=node_map[w].count)

    # é¢„æµ‹
    som_preds = []
    for x in feat_train:
        w = som.winner(x)
        som_preds.append(node_map.get(w, 0))  # é»˜è®¤0ç±»ä»¥é˜²ä¸‡ä¸€
    som_acc = accuracy_score(y_train, som_preds)

    print(f"    [å¯¹æ¯”ç»“æœ] Transformer Head Acc: {trans_acc:.2%} | SOM Acc: {som_acc:.2%}")

    print(">>> [5/7] ç”Ÿæˆæ ¸å¿ƒå¯è§†åŒ–å›¾è¡¨...")

    # å›¾ 1: è‡ªæ³¨æ„åŠ›çƒ­åŠ›å›¾ (Reds)
    plot_attention_heatmap(model, train_tensor, feature_cols)

    # å›¾ 2: SOM ç»„ä»¶å¹³é¢å›¾ (ä½¿ç”¨ Z-score æ•°æ® X_train)
    plot_som_component_planes(som, feat_train, X_train, feature_cols)

    # å›¾ 3: éšç©ºé—´å¯¹æ¯” (t-SNE)
    plot_latent_space_comparison(X_train, feat_train, y_train, class_names)

    # å›¾ 4: U-Matrix (å¸ƒå±€ä¼˜åŒ–)
    plot_u_matrix(som, feat_train, y_train, class_names)

    print(">>> [6/7] ç”Ÿæˆæ¶ˆèå®éªŒå¯¹æ¯”å›¾...")
    # å›¾ 5: æ¶ˆèå¯¹æ¯”
    plot_ablation_study(trans_acc, som_acc)

    print(f"\nğŸ‰ğŸ‰ğŸ‰ å…¨éƒ¨å®Œæˆï¼\nè¯·æ‰“å¼€æ–‡ä»¶å¤¹ '{OUTPUT_DIR}' æŸ¥çœ‹ä½ çš„ 5 å¼ è®ºæ–‡é…å›¾ã€‚")


if __name__ == "__main__":
    main()